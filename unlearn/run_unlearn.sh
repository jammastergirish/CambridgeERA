#!/usr/bin/env bash
# Run the unlearning pipeline for a given method.
#
# The output directory is auto-generated by unlearn.py based on the method
# and all relevant hyperparameters, so different runs never overwrite each other.
#
# Usage:
#   ./unlearn/run_unlearn.sh cb_lat                    # defaults
#   EPOCHS=2 ./unlearn/run_unlearn.sh cb_lat           # -> cb_lat__ep2_lr1e-05_bs4_...
#   EPOCHS=2 LR=5e-6 ./unlearn/run_unlearn.sh cb_lat  # -> cb_lat__ep2_lr5e-06_bs4_...
#
# Available methods:
#   ga_simple, ga, grad_diff, dpo, npo, simnpo, rmu, cb, lat, cb_lat,
#   tar, wt_dist, wt_dist_reg
#
# Environment overrides:
#   LR, EPOCHS, BATCH_SIZE, MAX_LENGTH, BETA, ALPHA, STEERING_COEFF,
#   LAYER_ID, FORGET_WEIGHT, RETAIN_WEIGHT, LAT_EPS, LAT_STEPS,
#   TAR_ALPHA, TAR_LR, TAR_EPOCHS, WT_NOISE_STD, WT_REG_LAMBDA,
#   GRAD_ACCUM_STEPS, EVAL_SPLIT, PUSH_TO_HUB, NO_SAVE, NO_EVAL
#
# For parallel multi-GPU sweeps, use parallel_sweep.sh instead of calling
# this script directly from a sweep loop.
set -euo pipefail

METHOD="${1:?Usage: $0 <ga_simple|ga|grad_diff|dpo|npo|simnpo|rmu|cb|lat|cb_lat|tar|wt_dist|wt_dist_reg>}"
BASE="${BASE:-EleutherAI/deep-ignorance-unfiltered}"
DEVICE="${DEVICE:-auto}"
DTYPE="${DTYPE:-auto}"

echo "=== Unlearning: method=${METHOD}  model=${BASE} ==="

# Check if this exact sweep config has already finished in W&B
if uv run --script unlearn/unlearn.py \
  --model "$BASE" \
  --method "$METHOD" \
  --forget-data data/forget.txt \
  --retain-data data/retain.txt \
  --device "$DEVICE" \
  --dtype "$DTYPE" \
  ${LR:+--lr "$LR"} \
  ${EPOCHS:+--epochs "$EPOCHS"} \
  ${BATCH_SIZE:+--batch-size "$BATCH_SIZE"} \
  ${MAX_LENGTH:+--max-length "$MAX_LENGTH"} \
  ${BETA:+--beta "$BETA"} \
  ${ALPHA:+--alpha "$ALPHA"} \
  ${STEERING_COEFF:+--steering-coeff "$STEERING_COEFF"} \
  ${LAYER_ID:+--layer-id "$LAYER_ID"} \
  ${FORGET_WEIGHT:+--forget-weight "$FORGET_WEIGHT"} \
  ${RETAIN_WEIGHT:+--retain-weight "$RETAIN_WEIGHT"} \
  ${LAT_EPS:+--lat-eps "$LAT_EPS"} \
  ${LAT_STEPS:+--lat-steps "$LAT_STEPS"} \
  ${TAR_ALPHA:+--tar-alpha "$TAR_ALPHA"} \
  ${TAR_LR:+--tar-lr "$TAR_LR"} \
  ${TAR_EPOCHS:+--tar-epochs "$TAR_EPOCHS"} \
  ${WT_NOISE_STD:+--wt-noise-std "$WT_NOISE_STD"} \
  ${WT_REG_LAMBDA:+--wt-reg-lambda "$WT_REG_LAMBDA"} \
  ${GRAD_ACCUM_STEPS:+--grad-accum-steps "$GRAD_ACCUM_STEPS"} \
  ${EVAL_SPLIT:+--eval-split "$EVAL_SPLIT"} \
  ${PUSH_TO_HUB:+--push-to-hub} \
  ${NO_SAVE:+--no-save} \
  ${NO_EVAL:+--no-eval} \
  --seed 42 \
  --check-wandb-only; then
  echo "=== Skipping ${METHOD}: Already finished successfully in W&B ==="
  exit 0
fi

uv run --script unlearn/unlearn.py \
  --model "$BASE" \
  --method "$METHOD" \
  --forget-data data/forget.txt \
  --retain-data data/retain.txt \
  --device "$DEVICE" \
  --dtype "$DTYPE" \
  ${LR:+--lr "$LR"} \
  ${EPOCHS:+--epochs "$EPOCHS"} \
  ${BATCH_SIZE:+--batch-size "$BATCH_SIZE"} \
  ${MAX_LENGTH:+--max-length "$MAX_LENGTH"} \
  ${BETA:+--beta "$BETA"} \
  ${ALPHA:+--alpha "$ALPHA"} \
  ${STEERING_COEFF:+--steering-coeff "$STEERING_COEFF"} \
  ${LAYER_ID:+--layer-id "$LAYER_ID"} \
  ${FORGET_WEIGHT:+--forget-weight "$FORGET_WEIGHT"} \
  ${RETAIN_WEIGHT:+--retain-weight "$RETAIN_WEIGHT"} \
  ${LAT_EPS:+--lat-eps "$LAT_EPS"} \
  ${LAT_STEPS:+--lat-steps "$LAT_STEPS"} \
  ${TAR_ALPHA:+--tar-alpha "$TAR_ALPHA"} \
  ${TAR_LR:+--tar-lr "$TAR_LR"} \
  ${TAR_EPOCHS:+--tar-epochs "$TAR_EPOCHS"} \
  ${WT_NOISE_STD:+--wt-noise-std "$WT_NOISE_STD"} \
  ${WT_REG_LAMBDA:+--wt-reg-lambda "$WT_REG_LAMBDA"} \
  ${GRAD_ACCUM_STEPS:+--grad-accum-steps "$GRAD_ACCUM_STEPS"} \
  ${EVAL_SPLIT:+--eval-split "$EVAL_SPLIT"} \
  ${PUSH_TO_HUB:+--push-to-hub} \
  ${NO_SAVE:+--no-save} \
  ${NO_EVAL:+--no-eval} \
  --seed 42
